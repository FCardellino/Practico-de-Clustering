{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/fernando/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import sys\n",
    "import os\n",
    "import vsm1 as vsm\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing1 import build_cooccurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso 1: Preprocesamiento del corpus\n",
    "def corpus_iterator(corpus_file):\n",
    "    document = {\n",
    "        \"title\": None,\n",
    "        \"body\": None\n",
    "    }\n",
    "\n",
    "    with open(corpus_file, \"r\") as fh:\n",
    "        for line in fh:\n",
    "            if line.strip() == \"-\":\n",
    "                new_document = True\n",
    "                document = {\n",
    "                    \"title\": None,\n",
    "                    \"body\": None\n",
    "                }\n",
    "            elif new_document:\n",
    "                document[\"title\"] = line.strip()\n",
    "                new_document = False\n",
    "            else:\n",
    "                document[\"body\"] = line.strip()\n",
    "                yield document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12947it [45:01,  4.92it/s]\n"
     ]
    }
   ],
   "source": [
    "#paso 1: Preprocesamiento del corpus\n",
    "nlp = spacy.load(\"es\")\n",
    "nlp.Defaults.stop_words |= {\"a\",\"e\",'y','o','u'}\n",
    "with open(\"./lavozprocess.conll\", \"w\") as fh:\n",
    "    for document in tqdm(corpus_iterator(\"./lavoztextodump1.txt\")):\n",
    "        for sentence in sent_tokenize(document[\"body\"], language=\"spanish\"):\n",
    "            for token in nlp(sentence):\n",
    "                if len(token) == 1:\n",
    "                    continue\n",
    "                if token.is_stop:\n",
    "                    continue\n",
    "                if not token.is_alpha:\n",
    "                    continue\n",
    "                print(token.text.lower(), token.lemma_, token.pos_, token.dep_, token.head,\n",
    "                      file=fh)\n",
    "            print(file=fh)\n",
    "        print(\"=\"*80, file=fh)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clara', 'crespo', 'rodolfo', 'martínez', 'imaginan']\n"
     ]
    }
   ],
   "source": [
    "#paso 1: Preprocesamiento del corpus\n",
    "def conll_iterator(file):\n",
    "    context = []\n",
    "    with open(file, \"r\") as fh:\n",
    "        for line in fh:\n",
    "            if line.strip() == \"\":\n",
    "                yield context\n",
    "                context = []\n",
    "            elif line.strip() == (\"=\" * 80):\n",
    "                continue\n",
    "            else:\n",
    "                (token, lemma, pos, dep, head) = line.strip().split()\n",
    "                context.append(token)\n",
    "\n",
    "for corpus in conll_iterator(\"./lavozprocess.conll\"):\n",
    "    print(corpus)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = []\n",
    "with open(\"./lavozprocess.conll\", \"r\") as fh:\n",
    "    for line in fh:\n",
    "        if line.strip() == \"\":\n",
    "            continue\n",
    "        elif line.strip() == (\"=\" * 80):\n",
    "            continue\n",
    "        else:\n",
    "            (token, lemma, pos, dep, head) = line.strip().split()\n",
    "            context.append(token)\n",
    "\n",
    "key_words = {}\n",
    "wid = 0\n",
    "for d in context:\n",
    "    if len(d) > 0:\n",
    "        key_words[d] = wid\n",
    "        wid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso 2: representación vectorial de las palabras y diseño de modelo de matriz\n",
    "lavoz_5window_scaled = build_cooccurrence_matrix(\n",
    "    corpus=conll_iterator(\"./lavozprocess.conll\"),\n",
    "    window_size=5,\n",
    "    scale_factor=\"scaled\",\n",
    "    vocab_size=5000,\n",
    "    unkown_vector=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abajo</th>\n",
       "      <th>abandonar</th>\n",
       "      <th>abandono</th>\n",
       "      <th>abastecimiento</th>\n",
       "      <th>abierta</th>\n",
       "      <th>abiertas</th>\n",
       "      <th>abierto</th>\n",
       "      <th>abiertos</th>\n",
       "      <th>abogado</th>\n",
       "      <th>abogados</th>\n",
       "      <th>...</th>\n",
       "      <th>índices</th>\n",
       "      <th>ómnibus</th>\n",
       "      <th>órdenes</th>\n",
       "      <th>órgano</th>\n",
       "      <th>órganos</th>\n",
       "      <th>única</th>\n",
       "      <th>único</th>\n",
       "      <th>únicos</th>\n",
       "      <th>útil</th>\n",
       "      <th>UNK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abajo</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandonar</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandono</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abastecimiento</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abierta</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                abajo  abandonar  abandono  abastecimiento  abierta  abiertas  \\\n",
       "abajo             0.5        0.0       0.2        0.000000      0.0       0.0   \n",
       "abandonar         0.0        0.0       0.0        0.000000      0.0       0.0   \n",
       "abandono          0.2        0.0       0.0        0.000000      0.0       0.0   \n",
       "abastecimiento    0.0        0.0       0.0        0.666667      0.0       0.0   \n",
       "abierta           0.0        0.0       0.0        0.000000      0.0       0.0   \n",
       "\n",
       "                abierto  abiertos  abogado  abogados  ...  índices  ómnibus  \\\n",
       "abajo              0.00       0.0      0.0       0.0  ...      0.0      0.0   \n",
       "abandonar          0.00       0.0      0.0       0.0  ...      0.0      0.0   \n",
       "abandono           0.00       0.0      0.0       0.0  ...      0.0      0.0   \n",
       "abastecimiento     0.00       0.0      0.0       0.0  ...      0.0      0.0   \n",
       "abierta            0.75       0.0      0.0       0.0  ...      0.0      0.0   \n",
       "\n",
       "                órdenes  órgano  órganos     única     único  únicos  útil  \\\n",
       "abajo               0.0     0.0      0.0  1.000000  0.000000     0.0   0.0   \n",
       "abandonar           0.0     0.0      0.0  0.000000  0.333333     0.0   0.0   \n",
       "abandono            0.0     0.0      0.0  0.000000  0.333333     0.0   0.0   \n",
       "abastecimiento      0.0     0.0      0.0  0.333333  0.000000     0.0   0.0   \n",
       "abierta             0.0     0.0      0.0  0.000000  0.000000     0.0   0.0   \n",
       "\n",
       "                     UNK  \n",
       "abajo           0.250000  \n",
       "abandonar       0.250000  \n",
       "abandono        1.000000  \n",
       "abastecimiento  0.333333  \n",
       "abierta         0.200000  \n",
       "\n",
       "[5 rows x 5001 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lavoz_5window_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso 2: representación vectorial de las palabras y diseño de modelo de matriz\n",
    "lavoz_20window_scaled = build_cooccurrence_matrix(\n",
    "    corpus=conll_iterator(\"./lavozprocess.conll\"),\n",
    "    window_size=20,\n",
    "    scale_factor=\"scaled\",\n",
    "    vocab_size=5000,\n",
    "    unkown_vector=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso 2: representación vectorial de las palabras y diseño de modelo de matriz\n",
    "\n",
    "#fout= open('./lavozconlliterado.txt', 'w')\n",
    "#for corpus in conll_iterator(\"./lavozprocess.conll\"):\n",
    "#    print(corpus, file=fout)\n",
    "\n",
    "\n",
    "#from sklearn.feature_extraction import DictVectorizer\n",
    "#v = DictVectorizer(sparse=False)\n",
    "#matrix = v.fit_transform(\"./lavozconlliterado.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#paso 3: medida de la distancia de las palabras \n",
    "#Explorar al vecino medainte distancia conseno\n",
    "vsm.neighbors(\"Argentina\", lavoz_5window_scaled, distfunc=vsm.cosine).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#paso 3: medida de la distancia de las palabras\n",
    "#Explorar al vecino mediante diatancia euclídea\n",
    "vsm.neighbors(\"Argentina\", lavoz_5window_scaled, distfunc=vsm.euclidean).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso 3: medida de la distancia de las palabras\n",
    "#método de reponderación observado/esperado\n",
    "lavoz_oe = vsm.observed_over_expected(lavoz_5window_scaled)\n",
    "vsm.neighbors(\"Argentina\", lavoz_oe).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso 3: medida de la distancia de las palabras\n",
    "#método de reponderación TfIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "corpus = lavoz_5window_scaled\n",
    "vectorized_corpus = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso 3: medida de la distancia de las palabras\n",
    "#método de reponderación PMI\n",
    "lavoz_ppmi = vsm.pmi(lavoz_5window_scaled, positive=True)\n",
    "vsm.neighbors(\"Argentina\", lavoz_ppmi).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#paso 3: medida de la distancia de las palabras\n",
    "#método de reducción de dimensionalidad LSA\n",
    "lavoz_lsa = vsm.lsa(lavoz_5window_scaled, k=100)\n",
    "#vsm.neighbors(\"Argentina\", lavoz_lsa).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso 4: clusterizar\n",
    "\n",
    "#from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def clustering2(k):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(lavoz_lsa)\n",
    "    clusters = kmeans.predict(lavoz_lsa)\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=20, random_state=0).fit(lavoz_lsa)\n",
    "\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c10= clustering2(10)\n",
    "printer = [word for word in key_words if c10[key_words[word]] == c10[key_words[\"abrir\"]]]\n",
    "print(printer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paso 4: clusterizar\n",
    "def clustering(k):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(vectorized_corpus.T)\n",
    "    clusters = kmeans.predict(vectorized_corpus.T)\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c50 =  clustering2(50)\n",
    "c50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2376352 is out of bounds for axis 0 with size 5001",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-84c7242cd461>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprinter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_words\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc50\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mc50\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"internacional\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprinter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-84c7242cd461>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprinter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_words\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc50\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mc50\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"internacional\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprinter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2376352 is out of bounds for axis 0 with size 5001"
     ]
    }
   ],
   "source": [
    "printer = [word for word in key_words if c50[key_words[word]] == c50[key_words[\"internacional\"]]]\n",
    "print(printer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c150 =  clustering(150)\n",
    "c150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printer = [word for word in vectorizer.vocabulary_ if c150[vectorizer.vocabulary_[word]] == c150[vectorizer.vocabulary_[\"internacional\"]]]\n",
    "print(printer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c300 =  clustering(300)\n",
    "c300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printer = [word for word in vectorizer.vocabulary_ if c300[vectorizer.vocabulary_[word]] == c300[vectorizer.vocabulary_[\"internacional\"]]]\n",
    "print(printer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printer = [word for word in vectorizer.vocabulary_ if c50[vectorizer.vocabulary_[word]] == c50[vectorizer.vocabulary_[\"argentina\"]]]\n",
    "print(printer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printer = [word for word in vectorizer.vocabulary_ if c150[vectorizer.vocabulary_[word]] == c150[vectorizer.vocabulary_[\"argentina\"]]]\n",
    "print(printer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printer = [word for word in vectorizer.vocabulary_ if c300[vectorizer.vocabulary_[word]] == c300[vectorizer.vocabulary_[\"argentina\"]]]\n",
    "print(printer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
